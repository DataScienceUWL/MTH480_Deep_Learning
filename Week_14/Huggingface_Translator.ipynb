{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3894c4d-4feb-47b4-882d-c634099ab792",
   "metadata": {},
   "source": [
    "Training an encoder-decoder model for translation using a dataset like Tatoeba, specifically translating from English to Spanish, can be a great way to get hands-on experience with sequence-to-sequence models. Below, I'll outline the steps to set up and train such a model using popular machine learning libraries like PyTorch and Hugging Face's Transformers.\n",
    "\n",
    "### Step 1: Setup Your Environment\n",
    "\n",
    "First, ensure you have Python installed and create a virtual environment to manage your dependencies. Then, install the necessary libraries.  You likely already have torch and numpy, but it may be neceesary to install the Hugging Face libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62e48e-b4c7-49ef-ae3d-c63f6530e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentencepiece accelerate sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fc549-b917-4713-90d1-e698bad6f036",
   "metadata": {},
   "source": [
    "### Step 2: Download and Prepare the Data\n",
    "\n",
    "You can use the `datasets` library from Hugging Face to load the [Tatoeba dataset](https://tatoeba.org/en/) filtered for English-Spanish pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfcd873-8deb-4a56-9dc4-fd12754d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset for English to Spanish (must use \"en\" and \"es\" not \"eng\" and \"spa\")\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"es\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5323dee-b4c5-45ce-b8fd-44b27cc8de36",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Data\n",
    "\n",
    "You'll need to tokenize the text data, convert it to tensor format, and create data loaders for training and evaluation. Here’s how you could set up the tokenizer and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a25b7b-59ed-48a9-a891-38059cee5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"es\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_test_split = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Combine the splits into a single DatasetDict\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e793819-9a48-4e7b-bc33-d391cbc9fa6a",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Model\n",
    "\n",
    "You will use a pretrained model that is suitable for translation. For English to Spanish, you can use a model like \"Helsinki-NLP/opus-mt-en-es\" from Hugging Face's Model Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5dd8bb-7d46-4ed6-a9d4-cd00a8a49d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f65f8d-e09e-4d54-a3fc-29a35a524783",
   "metadata": {},
   "source": [
    "### Step 5: Define Training Arguments and Train the Model\n",
    "\n",
    "**You should probably skip the step for learning purposes.  Most of the transformer models are pretty large so even a few epochs for fine tuning will be relatively slow yet much faster than training from scratch.  This cell shows you how to do it if you want use a dataset which is significantly different than common text.**\n",
    "\n",
    "You can train your model using the `Trainer` API from Hugging Face. Set up your training arguments and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565f7605-074b-4666-903b-198c990fc51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='36135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   26/36135 00:20 < 8:37:24, 1.16 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39msplit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39msplit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "fine_tune = False\n",
    "\n",
    "if fine_tune: \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset[\"train\"],\n",
    "        eval_dataset=split_dataset[\"validation\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f9cb9-f14c-4e01-831a-fbfde2efac21",
   "metadata": {},
   "source": [
    "Let's print a few examples and predictions from the validation set to see how it appears to be working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed57b9fc-d3ff-44ac-a859-d6c0ca6a9807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Tom told me you're only planning on staying here for three days.\n",
      "Target: Tom me dijo que sólo planeas quedarte aquí tres días.\n",
      "Predicted Translation: Tom me dijo que sólo planeas quedarte aquí tres días.\n",
      "-----\n",
      "Input: Did you speak with your wife?\n",
      "Target: ¿Has hablado con tu esposa?\n",
      "Predicted Translation: ¿Hablaste con tu esposa?\n",
      "-----\n",
      "Input: You think that it will work?\n",
      "Target: ¿Piensas que funcionará?\n",
      "Predicted Translation: ¿Crees que funcionará?\n",
      "-----\n",
      "Input: That's not a cat. That's a dog.\n",
      "Target: No es un gato. Es un perro.\n",
      "Predicted Translation: Eso no es un gato, es un perro.\n",
      "-----\n",
      "Input: I'd like to drink some tea or coffee.\n",
      "Target: Querría tomar un poco de té o café.\n",
      "Predicted Translation: Me gustaría tomar un poco de té o café.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def print_translation_examples(n_examples=5):\n",
    "    \n",
    "    model.eval() \n",
    "    \n",
    "    # Randomly select examples from the validation set\n",
    "    examples = random.sample(list(split_dataset['validation']), n_examples)\n",
    "\n",
    "    for example in examples:\n",
    "        input_text = example['translation']['en']\n",
    "        target_text = example['translation']['es']\n",
    "\n",
    "        # Tokenize the input text and convert to tensor\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # Generate translation using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "\n",
    "        # Decode the model output to text\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Target: {target_text}\")\n",
    "        print(f\"Predicted Translation: {translated_text}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "# Call the function\n",
    "print_translation_examples(n_examples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25affe79-75ed-48fd-8ea8-05bd3a5bb65f",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate and Use the Model\n",
    "\n",
    "After training, you can use the model to translate new sentences and evaluate its performance on a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a1b9302-51c8-4c8d-8bef-729cbfc083b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase está terminada por hoy.\n"
     ]
    }
   ],
   "source": [
    "# Translate a new sentence\n",
    "inputs = tokenizer(\"Class is finished for today.\", return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_sentence = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23149c-1430-4782-8649-3e910d466c1a",
   "metadata": {},
   "source": [
    "This setup provides a complete workflow from data loading and preprocessing to training and using a machine translation model. Adjust the parameters and configurations based on your specific requirements and available computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6f000-e874-4e04-be8c-7cf14381b865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
