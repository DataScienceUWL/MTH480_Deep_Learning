{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3894c4d-4feb-47b4-882d-c634099ab792",
   "metadata": {},
   "source": [
    "Training an encoder-decoder model for translation using a dataset like Tatoeba, specifically translating from English to Spanish, can be a great way to get hands-on experience with sequence-to-sequence models. Below, I'll outline the steps to set up and train such a model using popular machine learning libraries like PyTorch and Hugging Face's Transformers.\n",
    "\n",
    "### Step 1: Setup Your Environment\n",
    "\n",
    "First, ensure you have Python installed and create a virtual environment to manage your dependencies. Then, install the necessary libraries.  You likely already have torch and numpy, but it may be neceesary to install the Hugging Face libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62e48e-b4c7-49ef-ae3d-c63f6530e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentencepiece accelerate sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fc549-b917-4713-90d1-e698bad6f036",
   "metadata": {},
   "source": [
    "### Step 2: Download and Prepare the Data\n",
    "\n",
    "You can use the `datasets` library from Hugging Face to load the [Tatoeba dataset](https://tatoeba.org/en/) filtered for English-Spanish pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfcd873-8deb-4a56-9dc4-fd12754d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset for English to Spanish (must use \"en\" and \"es\" not \"eng\" and \"spa\")\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"es\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5323dee-b4c5-45ce-b8fd-44b27cc8de36",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Data\n",
    "\n",
    "You'll need to tokenize the text data, convert it to tensor format, and create data loaders for training and evaluation. Hereâ€™s how you could set up the tokenizer and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a25b7b-59ed-48a9-a891-38059cee5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"es\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_test_split = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Combine the splits into a single DatasetDict\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e793819-9a48-4e7b-bc33-d391cbc9fa6a",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Model\n",
    "\n",
    "You will use a pretrained model that is suitable for translation. For English to Spanish, you can use a model like \"Helsinki-NLP/opus-mt-en-es\" from Hugging Face's Model Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5dd8bb-7d46-4ed6-a9d4-cd00a8a49d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f65f8d-e09e-4d54-a3fc-29a35a524783",
   "metadata": {},
   "source": [
    "### Step 5: Define Training Arguments and Train the Model\n",
    "\n",
    "**You should probably skip the step for learning purposes.  Most of the transformer models are pretty large so even a few epochs for fine tuning will be relatively slow yet much faster than training from scratch.  This cell shows you how to do it if you want use a dataset which is significantly different than common text.**\n",
    "\n",
    "You can train your model using the `Trainer` API from Hugging Face. Set up your training arguments and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565f7605-074b-4666-903b-198c990fc51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='36135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   26/36135 00:20 < 8:37:24, 1.16 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39msplit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39msplit_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/UWL-DL/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "fine_tune = False\n",
    "\n",
    "if fine_tune: \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset[\"train\"],\n",
    "        eval_dataset=split_dataset[\"validation\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f9cb9-f14c-4e01-831a-fbfde2efac21",
   "metadata": {},
   "source": [
    "Let's print a few examples and predictions from the validation set to see how it appears to be working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed57b9fc-d3ff-44ac-a859-d6c0ca6a9807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Tom told me you're only planning on staying here for three days.\n",
      "Target: Tom me dijo que sÃ³lo planeas quedarte aquÃ­ tres dÃ­as.\n",
      "Predicted Translation: Tom me dijo que sÃ³lo planeas quedarte aquÃ­ tres dÃ­as.\n",
      "-----\n",
      "Input: Did you speak with your wife?\n",
      "Target: Â¿Has hablado con tu esposa?\n",
      "Predicted Translation: Â¿Hablaste con tu esposa?\n",
      "-----\n",
      "Input: You think that it will work?\n",
      "Target: Â¿Piensas que funcionarÃ¡?\n",
      "Predicted Translation: Â¿Crees que funcionarÃ¡?\n",
      "-----\n",
      "Input: That's not a cat. That's a dog.\n",
      "Target: No es un gato. Es un perro.\n",
      "Predicted Translation: Eso no es un gato, es un perro.\n",
      "-----\n",
      "Input: I'd like to drink some tea or coffee.\n",
      "Target: QuerrÃ­a tomar un poco de tÃ© o cafÃ©.\n",
      "Predicted Translation: Me gustarÃ­a tomar un poco de tÃ© o cafÃ©.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def print_translation_examples(n_examples=5):\n",
    "    \n",
    "    model.eval() \n",
    "    \n",
    "    # Randomly select examples from the validation set\n",
    "    examples = random.sample(list(split_dataset['validation']), n_examples)\n",
    "\n",
    "    for example in examples:\n",
    "        input_text = example['translation']['en']\n",
    "        target_text = example['translation']['es']\n",
    "\n",
    "        # Tokenize the input text and convert to tensor\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # Generate translation using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "\n",
    "        # Decode the model output to text\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Target: {target_text}\")\n",
    "        print(f\"Predicted Translation: {translated_text}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "# Call the function\n",
    "print_translation_examples(n_examples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25affe79-75ed-48fd-8ea8-05bd3a5bb65f",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate and Use the Model\n",
    "\n",
    "After training, you can use the model to translate new sentences and evaluate its performance on a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a1b9302-51c8-4c8d-8bef-729cbfc083b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase estÃ¡ terminada por hoy.\n"
     ]
    }
   ],
   "source": [
    "# Translate a new sentence\n",
    "inputs = tokenizer(\"Class is finished for today.\", return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_sentence = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23149c-1430-4782-8649-3e910d466c1a",
   "metadata": {},
   "source": [
    "This setup provides a complete workflow from data loading and preprocessing to training and using a machine translation model. Adjust the parameters and configurations based on your specific requirements and available computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6f000-e874-4e04-be8c-7cf14381b865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
