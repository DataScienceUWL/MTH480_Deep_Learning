{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78581229-c87e-400a-82f5-a1116c48ae97",
   "metadata": {},
   "source": [
    "### English to Spanish Translation Using Hugging Face\n",
    "\n",
    "In this tutorial, you'll learn how to use the Hugging Face `transformers` and `datasets` libraries to perform English to Spanish translation. We'll use the Tatoeba dataset for example sentences and the Helsinki-NLP's pretrained model for translation tasks.\n",
    "\n",
    "#### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2373b3a0-ade0-4fc8-b5e8-ddb22a78d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: filelock in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469a1fc-94de-4bf9-83c5-c1dd10afb173",
   "metadata": {},
   "source": [
    "#### Step 1: Download the Tatoeba Dataset\n",
    "First, we need to download the Tatoeba dataset, which contains pairs of sentences in various languages, including English and Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ddbcd3-2788-412c-a7e7-7bd6c558158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Tatoeba dataset\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"es\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7f983-dd48-4adc-ae46-9921bf273399",
   "metadata": {},
   "source": [
    "#### Step 2: Load the Pretrained Translation Model\n",
    "We will use the Helsinki-NLP model trained for English to Spanish translation. This model is available on the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7fe6f0-53dd-48ae-ad9f-83f3d7a4435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c788324-acd3-4b36-86a1-a2fd1d0353fd",
   "metadata": {},
   "source": [
    "#### Step 3: Translate Sentences from the Dataset\n",
    "\n",
    "To demonstrate how the pre-trained model works, we'll translate a few examples from the Tatoeba training set. The data is a little different than the original training data so you should see some minor differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f567f446-ffea-4750-a4fd-4bdf7265b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English: Muiriel is 20 now.\n",
      "Target Spanish: Ahora, Muiriel tiene 20 años.\n",
      "Predicted Spanish: Muiriel ahora tiene 20 años.\n",
      "\n",
      "Input English: This is never going to end.\n",
      "Target Spanish: Esto no terminará jamás.\n",
      "Predicted Spanish: Esto nunca va a terminar.\n",
      "\n",
      "Input English: You're in better shape than I am.\n",
      "Target Spanish: Estás en mejor forma que yo.\n",
      "Predicted Spanish: Estás en mejor forma que yo.\n",
      "\n",
      "Input English: That won't happen.\n",
      "Target Spanish: Eso no acontecerá.\n",
      "Predicted Spanish: Eso no sucederá.\n",
      "\n",
      "Input English: I'll call them tomorrow when I come back.\n",
      "Target Spanish: Les llamaré mañana cuando regrese.\n",
      "Predicted Spanish: Los llamaré mañana cuando vuelva.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(text):\n",
    "    # Encode the text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(**encoded_text)\n",
    "\n",
    "    # Decode and return the translation\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Select a few sentences from the dataset (you can change these)\n",
    "ids = [10,20,30,40,50]\n",
    "examples = [dataset[\"train\"][\"translation\"][id] for id in ids]\n",
    "\n",
    "for example in examples:\n",
    "    print(\"Input English:\", example[\"en\"])\n",
    "    print(\"Target Spanish:\", example[\"es\"])\n",
    "    translated_text = translate(example[\"en\"])\n",
    "    print(\"Predicted Spanish:\", translated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce62b9-88e2-4a01-9d54-e86eb751d324",
   "metadata": {},
   "source": [
    "#### Step 4: Translate a New Sentence\n",
    "Finally, demonstrate how to use the model to translate a new sentence that isn't from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d8cea7c-8812-4b04-b7c3-0b701030fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Sentence: Hello, how are you today?\n",
      "Translation: Hola, ¿cómo estás hoy?\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Hello, how are you today?\"\n",
    "translation = translate(new_sentence)\n",
    "print(\"New Sentence:\", new_sentence)\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404a3a5-e73d-4637-877b-9339a4d85beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
