{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ccb3e2-1a26-418c-aa15-5e539686dbbf",
   "metadata": {
    "id": "d8ccb3e2-1a26-418c-aa15-5e539686dbbf"
   },
   "source": [
    "# Using an open-source model to generate embeddings\n",
    "\n",
    "Transformer models are the gold standard for NLP these days and [Hugging Face](https://huggingface.ai/) is the most prominent site for open-source NLP models.  I asked ChatGPT to tell me about the \"best\" open-source model for sentiment analysis on the IMDB dataset.  Here's what it had to say:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8bbc9-65c0-42af-abd5-c14647b53600",
   "metadata": {
    "id": "f3f8bbc9-65c0-42af-abd5-c14647b53600"
   },
   "source": [
    "Selecting the \"best\" open-source model for creating text embeddings, especially for a task like sentiment analysis on the IMDB dataset, depends on several factors including the balance between performance (accuracy) and computational efficiency (speed and memory usage). As of my last update, here are a few top contenders from the Hugging Face Transformers library, which is a popular choice for accessing pre-trained models:\n",
    "\n",
    "### 1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Pros**: BERT has set a new standard in NLP for a variety of tasks, including sentiment analysis. It's capable of understanding the context of words in search queries or sentences more effectively than previous models.\n",
    "- **Model to Try**: `bert-base-uncased` for a balance between size and performance, or `bert-large-uncased` for higher accuracy at the cost of computational resources.\n",
    "\n",
    "### 2. RoBERTa (Robustly optimized BERT approach)\n",
    "- **Pros**: RoBERTa builds on BERT's language masking strategy, training the model more extensively on a larger dataset. It often outperforms BERT on several benchmarks.\n",
    "- **Model to Try**: `roberta-base` or `roberta-large`.\n",
    "\n",
    "### 3. DistilBERT\n",
    "- **Pros**: Offers a good trade-off between performance and resource efficiency. DistilBERT is a smaller, faster, cheaper, and lighter version of BERT.\n",
    "- **Model to Try**: `distilbert-base-uncased`.\n",
    "\n",
    "### 4. ALBERT (A Lite BERT)\n",
    "- **Pros**: ALBERT modifies BERT by factorizing the embedding layer and sharing parameters across the hidden layers, significantly reducing memory consumption and increasing training speed.\n",
    "- **Model to Try**: `albert-base-v2` or `albert-large-v2`.\n",
    "\n",
    "### 5. XLNet\n",
    "- **Pros**: XLNet outperforms BERT on several benchmarks by using a permutation-based training method, which allows it to capture bidirectional contexts effectively.\n",
    "- **Model to Try**: `xlnet-base-cased` or `xlnet-large-cased`.\n",
    "\n",
    "### 6. Sentence Transformers\n",
    "- **Pros**: Specifically designed for generating sentence embeddings, making it potentially more directly applicable for tasks like sentiment analysis. It uses Siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine similarity.\n",
    "- **Model to Try**: `all-MiniLM-L6-v2` for a balance between performance and speed, or `paraphrase-MiniLM-L6-v2` for high-quality embeddings suitable for various tasks including sentiment analysis.\n",
    "\n",
    "### Selection Criteria:\n",
    "- **Task and Data Specificity**: The nature of your task (e.g., sentiment analysis) and the characteristics of your data (e.g., length of texts, domain-specific language) may make some models more suitable than others.\n",
    "- **Computational Resources**: Larger models may offer better accuracy but require more computational power and longer inference times. Smaller models or distilled versions may be more practical for deployment, especially in resource-constrained environments.\n",
    "\n",
    "### Conclusion:\n",
    "For a balance between performance and computational efficiency, models like `distilbert-base-uncased`, `all-MiniLM-L6-v2`, or `paraphrase-MiniLM-L6-v2` from the Sentence Transformers library are strong candidates. These models provide a good starting point for creating embeddings for sentiment analysis on the IMDB dataset. Experimentation with a few models will help determine the best choice for your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bc91e-9a81-46ab-b0f8-e07ea3b1a525",
   "metadata": {
    "id": "879bc91e-9a81-46ab-b0f8-e07ea3b1a525"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "Below, we'll show how to use two of these models to create embeddings that can be used to train a simple classifier.  First, we'll show how use the `all-MiniLM-L6-v2` Sentence Transformer model.  Then we'll show how to use the `distilbert-base-uncased` transformer.  The first three steps are the same for either approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70fb36-3cf7-4df9-9976-7f82595cd233",
   "metadata": {
    "id": "2b70fb36-3cf7-4df9-9976-7f82595cd233"
   },
   "source": [
    "### Step 1: Install additional libraries and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39711e85-4d11-4758-9144-0c00362ee41e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96695,
     "status": "ok",
     "timestamp": 1712012467819,
     "user": {
      "displayName": "Jeff B",
      "userId": "12312427422906518493"
     },
     "user_tz": 300
    },
    "id": "39711e85-4d11-4758-9144-0c00362ee41e",
    "outputId": "83a015d5-630b-43f4-eaa5-6f73fc7f903f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/134.8 kB 330.3 kB/s eta 0:00:01\n",
      "     ---------------- -------------------- 61.4/134.8 kB 469.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 134.8/134.8 kB 890.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 42.0/42.0 kB 253.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/8.8 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/8.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/8.8 MB 3.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/8.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.9/8.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/8.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/8.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/8.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/8.8 MB 4.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/8.8 MB 4.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/8.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/8.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/8.8 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.1/8.8 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.3/8.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.5/8.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.6/8.8 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.9/8.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.0/8.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.1/8.8 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.3/8.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.3/8.8 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.4/8.8 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.4/8.8 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.8 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.8 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.6/8.8 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.8 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.8/8.8 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.8/8.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.9/8.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.9/8.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/8.8 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/8.8 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.1/8.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.2/8.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.3/8.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.3/8.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.4/8.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.5/8.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.5/8.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.6/8.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.7/8.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.7/8.8 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.8/8.8 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.8/8.8 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.8/8.8 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.9/8.8 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.9/8.8 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.0/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.0/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.1/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.1/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.1/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.2/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.2/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.3/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.3/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.3/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.4/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.4/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.5/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.5/8.8 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.6/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.8/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.0/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.0/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.2/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.2/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.3/8.8 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.5/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.7/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.7/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.8/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.9/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.0/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.3/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.5/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.6/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 71.7/388.9 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 194.6/388.9 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 286.7/388.9 kB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 378.9/388.9 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.9/388.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 102.4/269.5 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 225.3/269.5 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/269.6 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 235.5/269.6 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.22.2 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (4.39.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jbaggett\\mambaforge\\envs\\uwl-dl\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/163.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 61.4/163.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 163.3/163.3 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.6.1\n"
     ]
    }
   ],
   "source": [
    "# only need to do this once (per session)\n",
    "# !pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "#! pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cKsi2bUQ5dUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15638,
     "status": "ok",
     "timestamp": 1712012558521,
     "user": {
      "displayName": "Jeff B",
      "userId": "12312427422906518493"
     },
     "user_tz": 300
    },
    "id": "cKsi2bUQ5dUf",
    "outputId": "b18c42e1-a294-4e96-bc80-230f37067213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (1.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (4.10.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from pytorch_lightning) (0.10.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (69.1.1)\n",
      "Requirement already satisfied: filelock in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: torchmetrics in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchmetrics) (2.2.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchmetrics) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.1.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857db0a6-fd43-4a0f-abda-7a322153e67e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3868,
     "status": "ok",
     "timestamp": 1712012586241,
     "user": {
      "displayName": "Jeff B",
      "userId": "12312427422906518493"
     },
     "user_tz": 300
    },
    "id": "857db0a6-fd43-4a0f-abda-7a322153e67e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.datasets import IMDB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "from utils import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d4651-e468-4867-8ff2-0c9e87e2218a",
   "metadata": {
    "id": "773d4651-e468-4867-8ff2-0c9e87e2218a"
   },
   "source": [
    "### Step 2: Load the IMDB Dataset\n",
    "The torchdata library provides a simple API to load the IMDB dataset. We will use this dataset to train and evaluate our sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mTnX8iPc50fg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20480,
     "status": "ok",
     "timestamp": 1712012834026,
     "user": {
      "displayName": "Jeff B",
      "userId": "12312427422906518493"
     },
     "user_tz": 300
    },
    "id": "mTnX8iPc50fg",
    "outputId": "dbfba773-3cfd-4203-d86a-902c7a6307ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (2.8.2)\n",
      "Requirement already satisfied: torchdata in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchdata) (2.2.1)\n",
      "Requirement already satisfied: requests in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchdata) (2.2.1)\n",
      "Requirement already satisfied: filelock in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch>=2->torchdata) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchdata) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchdata) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Requirement already satisfied: torchtext in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: tqdm in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchtext) (4.66.2)\n",
      "Requirement already satisfied: requests in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchtext) (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: filelock in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torch==2.2.1->torchtext) (2024.2.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from torchdata==0.7.1->torchtext) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchtext) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jbaggett/miniforge3/envs/UWL-DL/lib/python3.11/site-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker\n",
    "!pip install torchdata\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e5642e-77a4-4c6d-9983-7b25eb0ab066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "error",
     "timestamp": 1712012795172,
     "user": {
      "displayName": "Jeff B",
      "userId": "12312427422906518493"
     },
     "user_tz": 300
    },
    "id": "e2e5642e-77a4-4c6d-9983-7b25eb0ab066",
    "outputId": "0afec3ba-f80e-4ad8-f706-a2ec9bcb5a71"
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# Convert to list for easier processing\n",
    "train_data = list(train_iter)\n",
    "test_data = list(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f428c2-d9f2-427f-aff2-a1523ffc5d0d",
   "metadata": {
    "id": "07f428c2-d9f2-427f-aff2-a1523ffc5d0d"
   },
   "source": [
    "### Step 3: Process the Labels\n",
    "\n",
    "Read the labels and convert them into tensors of 0 and 1 floats for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116a5357-713a-407f-ae94-ae988e297836",
   "metadata": {
    "id": "116a5357-713a-407f-ae94-ae988e297836"
   },
   "outputs": [],
   "source": [
    "def process_labels(data):\n",
    "    # The IMDB uses 1 for a negative review and 2 for a positive review\n",
    "    # We'll use 0 and 1, respectively\n",
    "    labels = [label-1 for label, _ in data]\n",
    "    return torch.tensor(labels).unsqueeze(1).float()\n",
    "\n",
    "train_labels = process_labels(train_data)\n",
    "test_labels = process_labels(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bea49-118d-4d5b-980c-f2274570e8e3",
   "metadata": {
    "id": "c63bea49-118d-4d5b-980c-f2274570e8e3"
   },
   "source": [
    "## Using the MiniLM Sentence Transformer\n",
    "\n",
    "Step 4 for loading the embedding model and generating the embeddings is a little differnt for each of the two models.  Here we'll show the MiniLM version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2994136-88a8-4812-a0b9-4a96a5c93c71",
   "metadata": {
    "id": "e2994136-88a8-4812-a0b9-4a96a5c93c71"
   },
   "source": [
    "### Step 4 (MiniLM): Load the Embedding Model and Generate Text Embeddings\n",
    "\n",
    "This step will change depending on which embedding model\n",
    "We will use a Sentence Transformers model to convert the IMDB reviews into embeddings. These embeddings will serve as input features for our neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8179e68d-40da-452d-9bd1-953dd0fcfe04",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e7dae2590c424c3cb0a858ebdcdcce31",
      "8bc06df08f174786ba5ef838a961c5c7",
      "f7332f98b497435eba0241fa9c441b1c",
      "14ec063084a2465887341ed914aac726",
      "7f8673371f9c4c09b43514f10c7f1a90",
      "774f0b7e89aa422890ef63a8bb5b3fe2",
      "83b3e766ee104958a4eff5d3998fd0df",
      "c558ed1318d849e5a59d4ec04f9ace00",
      "97e75160ed344fbb9184921363a20b55",
      "b51cb41c13a346d5a1bd27a09a9dbb99",
      "a6cf4fe3038a4a0b86f25692b2314021",
      "481ef82871ba4c7fa0132cb0fd954179",
      "5442ef14ba514de7a432fb292cb376e8"
     ]
    },
    "id": "8179e68d-40da-452d-9bd1-953dd0fcfe04",
    "outputId": "fe99b5cc-efe2-4c58-dd85-61649a33eb5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fab4b08b4d49cabb800208e5417dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbaggett\\mambaforge\\envs\\UWL-DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jbaggett\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd30cad8a095463ca7e37bfb5fd10d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e0112b6474014a31cade4f2a43f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fcf4df4c6c4f8daf36aedbc6cef5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d84a4b1a69e4eb2ba5e36f4ca5c16ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7f036889a94d299bc5757f9480c57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20338ff8b3cb4ed8af954acdb061cb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3350d15d9740e290118e07692c8960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219be67ba9784d36a9c82f3e634c8998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a93ae3429894a989d3657fed285b7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851113f4e0e34c06a17a0ec6b9ebd975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f5c04ca9274cc6b4f1b8ae34e40357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777eb125e3d34703851f827bf526bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Generate Embeddings\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "model_name = 'all-MiniLM-L6-v2'  # A good balance between performance and speed, 22M parameters\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    texts = [text for _, text in data]\n",
    "    embeddings = sentence_model.encode(texts, show_progress_bar=True)\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "train_embeddings = generate_embeddings(train_data)\n",
    "test_embeddings = generate_embeddings(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b2b4d-47a9-4f71-abfd-63845bbf4531",
   "metadata": {
    "id": "e71b2b4d-47a9-4f71-abfd-63845bbf4531"
   },
   "source": [
    "### Step 5:  Choose the classifier\n",
    "\n",
    "This step is the same for both embedding models, but the size of the embeddings may be different, so we'll create the class for our classifier here but instantiate it for each set of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3ab98f-57c3-4bcd-b3f5-6046764dc0bf",
   "metadata": {
    "id": "da3ab98f-57c3-4bcd-b3f5-6046764dc0bf"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim,128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed55c0-d7a8-438f-b117-99a135327dc2",
   "metadata": {
    "id": "b0ed55c0-d7a8-438f-b117-99a135327dc2"
   },
   "source": [
    "### Step 6 (MiniLM): Build and Train the Classifier\n",
    "\n",
    "This step is pretty similar for both embedding models, we need to build the datasets and loaders here since the embeddings will differ.  We'll also instantiate our classifier and train it.\n",
    "\n",
    "We'll use `train_model` from `utils` and the accuracy metric from `torchmetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb77e0e-6dfe-4d47-bcbb-8b4c5a1b4fc0",
   "metadata": {
    "id": "0cb77e0e-6dfe-4d47-bcbb-8b4c5a1b4fc0"
   },
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1751e902-72f0-454e-9034-b1605b534438",
   "metadata": {
    "id": "1751e902-72f0-454e-9034-b1605b534438",
    "outputId": "72b8679d-9bd9-44f0-ee75-29ba91560ee9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training 100.00% complete, Validation 100.00% complete lr = 1.000e-03\n",
      " Epoch  train_accuracy  train_loss  val_accuracy  val_loss      Time    LR\n",
      "     6         0.82456    0.387316       0.81504  0.401627 12.763296 0.001\n",
      "     7         0.82796    0.379597       0.81528  0.400027 13.656353 0.001\n",
      "     8         0.83276    0.372259       0.81432  0.400576 11.800880 0.001\n",
      "     9         0.83812    0.365090       0.80776  0.414728 11.708164 0.001\n",
      "    10         0.84372    0.356869       0.81340  0.402624 11.345714 0.001\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "input_dim = train_embeddings.shape[1]\n",
    "classifier = SentimentClassifier(input_dim)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "metrics = {\n",
    "    'accuracy': torchmetrics.Accuracy(num_classes=2, task='binary'),\n",
    "}\n",
    "\n",
    "minilm_results_df = train_model(classifier, loss_function,\n",
    "                         train_loader = train_loader,\n",
    "                         val_loader = test_loader,\n",
    "                         metrics=metrics,\n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba1636-07c4-452e-982a-641a30d2dbfa",
   "metadata": {
    "id": "b1ba1636-07c4-452e-982a-641a30d2dbfa"
   },
   "source": [
    "## Using the DistilBERT Transformer\n",
    "\n",
    "Here we modify Step 4 to load the embedding model and generate the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555f2fd-94c4-4300-9d24-e1f35eda1e23",
   "metadata": {
    "id": "3555f2fd-94c4-4300-9d24-e1f35eda1e23"
   },
   "source": [
    "### Step 4 (DistilBERT): Load the Embedding Model and Generate Text Embeddings\n",
    "\n",
    "This is pretty slow (it took slightly more than an hour on my Macbook CPU).  I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3dd5b7-7ab4-4c9d-a545-e0c84e65245e",
   "metadata": {
    "id": "7a3dd5b7-7ab4-4c9d-a545-e0c84e65245e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e0edd4c3f043debe0193fac2b0ba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbaggett\\mambaforge\\envs\\UWL-DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jbaggett\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df8d64d947342ec9c138710360db954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f3f16206dd4c5f8e635c69f1a97b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e798fd12547a4f2fb403f2049db2702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99fbb41a3274ff18bc4b65d44f64e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75216d2a-ddca-4a49-8264-52395668ccf8",
   "metadata": {
    "id": "75216d2a-ddca-4a49-8264-52395668ccf8",
    "outputId": "773f8fc3-adc8-4b9e-bf3c-512f6427b716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "def detect_device(use_MPS = False):\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and use_MPS:\n",
    "        print(\"MPS (Apple Silicon) is available\")\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Only cpu is available\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83e8c7d-1584-47ef-a00f-0a28976fe7cc",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3c8371c701174b6890415fb2a8c53544"
     ]
    },
    "id": "bce16bd5-5fe6-4d6f-a276-0b0342a767ba",
    "outputId": "f515046f-0d35-45de-b381-f939485ec0d7"
   },
   "source": [
    "from tqdm.auto import tqdm  # Automatically use notebook-friendly or console version\n",
    "model.to(device)\n",
    "\n",
    "def generate_embeddings(data, device, batch_size=16):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    # Prepare DataLoader for batch processing\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    progress_bar = tqdm(data_loader, desc='Generating Embeddings')\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for _,texts in progress_bar:\n",
    "            encoded_input = tokenizer(list(texts), padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "            output = model(**{k: v.to(device) for k, v in encoded_input.items()})\n",
    "            batch_embeddings = output.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings\n",
    "\n",
    "train_embeddings = generate_embeddings(train_data, device)\n",
    "test_embeddings = generate_embeddings(test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e62e9e0f-8b63-41cf-bb51-6cbddfdb714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1889baddd1914438ba8386a27f04e029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e046b6cd29714c8eaf2377522538f9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # Automatically use notebook-friendly or console version\n",
    "model.to(device)\n",
    "\n",
    "def generate_embeddings(data, device, batch_size=16):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    cls_embeddings = []\n",
    "    mean_embeddings = []\n",
    "\n",
    "    # Prepare DataLoader for batch processing\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    progress_bar = tqdm(data_loader, desc='Generating Embeddings')\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for _,texts in progress_bar:\n",
    "            encoded_input = tokenizer(list(texts), padding=True, truncation=True, \n",
    "                                      return_tensors='pt', max_length=512,\n",
    "                                      add_special_tokens=True\n",
    "                                     ).to(device)\n",
    "            output = model(**{k: v.to(device) for k, v in encoded_input.items()})\n",
    "            batch_mean_embeddings = output.last_hidden_state.mean(dim=1)\n",
    "            mean_embeddings.append(batch_mean_embeddings.cpu())\n",
    "            batch_cls_embeddings = output.last_hidden_state[:,0,:]\n",
    "            cls_embeddings.append(batch_cls_embeddings.cpu())\n",
    "\n",
    "    # Concatenate all batch embeddings\n",
    "    mean_embeddings = torch.cat(mean_embeddings, dim=0)\n",
    "    cls_embeddings = torch.cat(cls_embeddings, dim=0)\n",
    "    return mean_embeddings, cls_embeddings\n",
    "\n",
    "train_mean_embeddings, train_cls_embeddings = generate_embeddings(train_data, device)\n",
    "test_mean_embeddings, test_cls_embeddings = generate_embeddings(test_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51667ea-b1c8-49e4-8439-cc6b3d7facd4",
   "metadata": {
    "id": "c51667ea-b1c8-49e4-8439-cc6b3d7facd4"
   },
   "source": [
    "### Step 6 (DistilBERT): Build and Train the Classifier\n",
    "\n",
    "This step is the same as before, but we instantiate the datasets and loaders again since the embeddings have changed.  We'll do this twice to see if there is a difference in the results depending on how the final embedding is formed.\n",
    "\n",
    "#### First we'll use mean emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75c04b37-e90e-4f6e-abd4-e46dac1f154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cls_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f5caf3d-a17e-44c8-8902-a3d5b34dd7d7",
   "metadata": {
    "id": "0f5caf3d-a17e-44c8-8902-a3d5b34dd7d7"
   },
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_mean_embeddings, train_labels)\n",
    "test_dataset = TensorDataset(test_mean_embeddings, test_labels)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3a5a35e-e5d5-44b1-b16d-7bd4ad118093",
   "metadata": {
    "id": "a3a5a35e-e5d5-44b1-b16d-7bd4ad118093",
    "outputId": "b5a9d656-a748-4a90-c9e6-1295ab951cdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training 100.00% complete, Validation 100.00% complete lr = 1.000e-03\n",
      " Epoch  train_accuracy  train_loss  val_accuracy  val_loss      Time    LR\n",
      "     6         0.87916    0.292371       0.87532  0.290044 14.094957 0.001\n",
      "     7         0.87760    0.288965       0.87688  0.288059 13.502207 0.001\n",
      "     8         0.88308    0.283877       0.87840  0.286740 13.782764 0.001\n",
      "     9         0.88236    0.281740       0.87916  0.283231 13.856043 0.001\n",
      "    10         0.88668    0.276065       0.87460  0.293534 14.869065 0.001\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "input_dim = train_mean_embeddings.shape[1]\n",
    "classifier = SentimentClassifier(input_dim)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "metrics = {\n",
    "    'accuracy': torchmetrics.Accuracy(num_classes=2, task='binary'),\n",
    "}\n",
    "\n",
    "distilbert_mean_results_df = train_model(classifier, loss_function,\n",
    "                         train_loader = train_loader,\n",
    "                         val_loader = test_loader,\n",
    "                         metrics=metrics,\n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a57f8-1707-4863-b6ae-148385341e74",
   "metadata": {},
   "source": [
    "#### Second we'll try the CLS token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "337c25c0-7905-4628-910d-b05aaac7e29b",
   "metadata": {
    "id": "0f5caf3d-a17e-44c8-8902-a3d5b34dd7d7"
   },
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_cls_embeddings, train_labels)\n",
    "test_dataset = TensorDataset(test_cls_embeddings, test_labels)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d805f059-fc49-4651-9660-43d6e996eadb",
   "metadata": {
    "id": "a3a5a35e-e5d5-44b1-b16d-7bd4ad118093",
    "outputId": "b5a9d656-a748-4a90-c9e6-1295ab951cdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training 100.00% complete, Validation 100.00% complete lr = 1.000e-03\n",
      " Epoch  train_accuracy  train_loss  val_accuracy  val_loss      Time    LR\n",
      "     6         0.87200    0.302358       0.87160  0.299234 14.741745 0.001\n",
      "     7         0.87408    0.300601       0.87248  0.301044 13.555089 0.001\n",
      "     8         0.87748    0.294722       0.86592  0.310991 12.489838 0.001\n",
      "     9         0.87624    0.292098       0.87056  0.302251 11.399477 0.001\n",
      "    10         0.87712    0.291831       0.87420  0.294464 13.069873 0.001\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "input_dim = train_mean_embeddings.shape[1]\n",
    "classifier = SentimentClassifier(input_dim)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "metrics = {\n",
    "    'accuracy': torchmetrics.Accuracy(num_classes=2, task='binary'),\n",
    "}\n",
    "\n",
    "distilbert_mean_results_df = train_model(classifier, loss_function,\n",
    "                         train_loader = train_loader,\n",
    "                         val_loader = test_loader,\n",
    "                         metrics=metrics,\n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab938f-ad45-40dc-a785-3e9d153e0050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
