{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a92dec-c398-435f-aa32-2fe2ccf95b03",
   "metadata": {},
   "source": [
    "# Binary Classifcation - Evens and Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618304ae-062d-4b17-8785-0504f0bd7d03",
   "metadata": {},
   "source": [
    "We'll demonstrate a few things here.\n",
    "\n",
    "* split the original MNIST training set into training and validation sets\n",
    "* build a custom dataloader to return labels of 0 for even digits and 1 for odd digits\n",
    "* make predictions and assess metrics on the test set\n",
    "* our simple CNN will output a single number (a logit from the output layer)\n",
    "* we'll use BCEWithLogitsLoss since the outputs can be any real numbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b61e996-a1dc-413f-8df4-4a2deadb0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import train_model # you need to copy utils.py to your working directory in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df075f7-4538-4398-bc57-f1e9abe087c8",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00c8dcc0-be88-44dd-99cf-190a1393ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        # Convert label to 0 for even digits and 1 for odd digits\n",
    "        label = np.float32(label % 2) # floats necessary for BCEWithLogitsLoss\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77874299-244d-42c0-8ba5-8420a8d550d5",
   "metadata": {},
   "source": [
    "### Extra Split and Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967261ee-8ec5-46b1-b02b-e830c210d579",
   "metadata": {},
   "source": [
    "Here we split the MNIST training set into a smaller training set (80%) and a validation set (20%) so we can monitor our metrics during training and use those metrics to help us choose models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c506c59-4fe6-4131-bef3-45d356f0ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split into training and validation\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "validation_size = len(full_train_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Wrap the training and validation datasets using CustomMNISTDataset\n",
    "train_dataset = CustomMNISTDataset(train_dataset)\n",
    "validation_dataset = CustomMNISTDataset(validation_dataset)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_dataset = CustomMNISTDataset(test_dataset)\n",
    "\n",
    "# Loaders\n",
    "B = 100  # batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=B, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_dataset, batch_size=B, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=B, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bd8d2-7f0f-4a83-a47b-1ec742c22e6d",
   "metadata": {},
   "source": [
    "### Define a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f1d81-f2b2-44b1-8cfd-56fc3cec499c",
   "metadata": {},
   "source": [
    "I copied this model from one of our CIFAR10 experiments and tweaked it to have one input channel and a single output class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70b243b5-a87a-4a07-b5d6-cc49f5dde4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(in_chan, out_chan, kernel_size = 3, stride = 1, pad = 2):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d( in_chan, out_chan, kernel_size, stride, pad),\n",
    "        nn.BatchNorm2d( out_chan ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)      \n",
    "    )\n",
    "    return block\n",
    "\n",
    "def ClassificationHead1H(in_chan, hidden_chan, num_outputs):\n",
    "    head = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_chan, hidden_chan),\n",
    "        nn.BatchNorm1d(hidden_chan),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_chan , num_outputs)\n",
    "    )\n",
    "    return head\n",
    "            \n",
    "\n",
    "class EvenOddNet(nn.Module):\n",
    "    '''\n",
    "    Two convolution layers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            ConvBlock(1, 8),\n",
    "            ConvBlock(8,16),\n",
    "            ConvBlock(16,32),\n",
    "            ClassificationHead1H(32*5*5,100,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.layers(x).squeeze()) # squeeze that single dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bd6e875-17a4-4c60-859f-7e49d1a75d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "EvenOddNet                               [64, 1, 28, 28]           [64]                      --\n",
       "├─Sequential: 1-1                        [64, 1, 28, 28]           [64, 1]                   --\n",
       "│    └─Sequential: 2-1                   [64, 1, 28, 28]           [64, 8, 15, 15]           --\n",
       "│    │    └─Conv2d: 3-1                  [64, 1, 28, 28]           [64, 8, 30, 30]           80\n",
       "│    │    └─BatchNorm2d: 3-2             [64, 8, 30, 30]           [64, 8, 30, 30]           16\n",
       "│    │    └─ReLU: 3-3                    [64, 8, 30, 30]           [64, 8, 30, 30]           --\n",
       "│    │    └─MaxPool2d: 3-4               [64, 8, 30, 30]           [64, 8, 15, 15]           --\n",
       "│    └─Sequential: 2-2                   [64, 8, 15, 15]           [64, 16, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-5                  [64, 8, 15, 15]           [64, 16, 17, 17]          1,168\n",
       "│    │    └─BatchNorm2d: 3-6             [64, 16, 17, 17]          [64, 16, 17, 17]          32\n",
       "│    │    └─ReLU: 3-7                    [64, 16, 17, 17]          [64, 16, 17, 17]          --\n",
       "│    │    └─MaxPool2d: 3-8               [64, 16, 17, 17]          [64, 16, 8, 8]            --\n",
       "│    └─Sequential: 2-3                   [64, 16, 8, 8]            [64, 32, 5, 5]            --\n",
       "│    │    └─Conv2d: 3-9                  [64, 16, 8, 8]            [64, 32, 10, 10]          4,640\n",
       "│    │    └─BatchNorm2d: 3-10            [64, 32, 10, 10]          [64, 32, 10, 10]          64\n",
       "│    │    └─ReLU: 3-11                   [64, 32, 10, 10]          [64, 32, 10, 10]          --\n",
       "│    │    └─MaxPool2d: 3-12              [64, 32, 10, 10]          [64, 32, 5, 5]            --\n",
       "│    └─Sequential: 2-4                   [64, 32, 5, 5]            [64, 1]                   --\n",
       "│    │    └─Flatten: 3-13                [64, 32, 5, 5]            [64, 800]                 --\n",
       "│    │    └─Linear: 3-14                 [64, 800]                 [64, 100]                 80,100\n",
       "│    │    └─BatchNorm1d: 3-15            [64, 100]                 [64, 100]                 200\n",
       "│    │    └─ReLU: 3-16                   [64, 100]                 [64, 100]                 --\n",
       "│    │    └─Linear: 3-17                 [64, 100]                 [64, 1]                   101\n",
       "===================================================================================================================\n",
       "Total params: 86,401\n",
       "Trainable params: 86,401\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 61.06\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 15.49\n",
       "Params size (MB): 0.35\n",
       "Estimated Total Size (MB): 16.03\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking to see if we got the sizes right!\n",
    "summary(EvenOddNet(), input_size = (64,1,28,28), col_names = [\"input_size\",\"output_size\",\"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebbf9c-da4b-4703-afc8-5ddd4673e33d",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Here we'll use the training and validation sets.  After the model is trained (and perhaps we've trained multiple models) we can evaluate our final model on the test set.\n",
    "\n",
    "Note we're using accuracy from torchmetrics to monitory accuracy during training, but we'll use accuracy_score from scikit-learn later to evaluate accuracy on the test set.  (I haven't had time to develop a complete end-to-end Pytorch Lightning solution using wrapper functions for predictions and metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8eb942f-c32a-4e08-885b-904fd73cebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training 100.00% complete, Validation 100.00% complete lr = 1.000e-03\n",
      " Epoch  train_accuracy  train_loss  val_accuracy  val_loss      Time    LR\n",
      "     1        0.975937    0.073754      0.986333  0.041177 14.864981 0.001\n",
      "     2        0.990417    0.028623      0.977750  0.060018 15.415805 0.001\n",
      "     3        0.992979    0.019641      0.992000  0.024382 15.438186 0.001\n",
      "     4        0.994625    0.015768      0.991417  0.025888 15.079498 0.001\n",
      "     5        0.995479    0.013277      0.992417  0.025299 15.525084 0.001\n"
     ]
    }
   ],
   "source": [
    "even_odd_model = EvenOddNet()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "num_epochs=5\n",
    "\n",
    "metrics = {'accuracy': torchmetrics.Accuracy(task='binary'),}\n",
    "\n",
    "results_df = train_model(even_odd_model, loss_function,\n",
    "                         epochs = num_epochs,\n",
    "                         metrics = metrics,\n",
    "                         train_loader = train_loader,\n",
    "                         val_loader = valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89206e0-1dc0-491e-8946-b2d9612bd309",
   "metadata": {},
   "source": [
    "### Make Predictions and Compute Metrics\n",
    "\n",
    "The approach here can be used with any PyTorch model and DataLoader.  We'll break it down into steps.  First we'll define a function to apply our model to all the batches in from the loader.  It accumulates the labels and outputs in two lists and then concatenates those lists to tensors before returning them.  The first dimension of each output tensor will have the same size as the dataset being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12f8ab57-6bda-41b5-b575-2d234587b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            all_outputs.append(outputs.cpu())  # Move outputs to CPU\n",
    "            all_labels.append(labels.cpu())  # Move labels to CPU\n",
    "    \n",
    "    # Concatenate all the outputs and labels across batches\n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_outputs, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272d8a5-d8f7-4893-b084-c2f7fb88b35a",
   "metadata": {},
   "source": [
    "Notice that the model returns the raw model outputs.  Here it is in action for the MNIST test_loader.  Make sure your test_loader uses shuffle=False so you can find the indices of particular results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66e64d8c-6032-4ce9-85e4-f450249dc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs,labels = predict_model(even_odd_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c49e818b-8a2e-4ac6-967c-6d07a07ad88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9.2326, -12.8338,   9.8103,  ..., -15.9064,  15.1934, -15.1220])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8768246a-f21b-44ae-bdcb-783a4746ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.,  ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636a6ba-791c-48dd-ae09-633cc1cbf1a4",
   "metadata": {},
   "source": [
    "Notice that the outputs need to be processed into predictions to be similar.  In this case our logits are single values that can be any real number.  To get predictions we need to apply a sigmoid and convert the result to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9717a793-37e1-448b-9738-a7da1c47a2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = (torch.sigmoid(outputs)>.5).long()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed80828-0456-45c9-9a60-0df1f7ee8dbf",
   "metadata": {},
   "source": [
    "If you think through the math for a bit, you could just figure out where the logits are postive to get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb71873a-fb7f-4c25-8ed0-ca461469b861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = (outputs>0).long()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fc375-872d-41f2-b045-06f217297876",
   "metadata": {},
   "source": [
    "#### If you used two classes in your model ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96810ab4-00a1-4d8a-a81e-60562cb61006",
   "metadata": {},
   "source": [
    "If your outputs were for two classes instead, your output tensor would have size Bx2 like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "706f43ab-116e-4343-b61f-6e976a139fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778],\n",
       "        [-0.3035, -0.5880],\n",
       "        [ 1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010],\n",
       "        [-0.1606, -0.4015],\n",
       "        [ 0.6957, -1.8061],\n",
       "        [-1.1589,  0.3255],\n",
       "        [-0.6315, -2.8400],\n",
       "        [-0.7849, -1.4096],\n",
       "        [-0.4076,  0.7953]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "B = 10\n",
    "out = torch.randn(B,2)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd3a7d-8bf9-4ecd-9981-623e5c99d167",
   "metadata": {},
   "source": [
    "The CrossEntropyLoss has a softmax built in that converts the numbers so that each output sums to 1, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c02fa74-3f77-405a-afea-156eb8ce129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6260, 0.3740],\n",
       "        [0.5706, 0.4294],\n",
       "        [0.5695, 0.4305],\n",
       "        [0.8140, 0.1860],\n",
       "        [0.5599, 0.4401],\n",
       "        [0.9243, 0.0757],\n",
       "        [0.1848, 0.8152],\n",
       "        [0.9010, 0.0990],\n",
       "        [0.6513, 0.3487],\n",
       "        [0.2310, 0.7690]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a72b0-b0a8-4a04-b71d-16dd545c3ae2",
   "metadata": {},
   "source": [
    "Then we choose the label corresponding to the largest value in each output.  So for the first output [0.6260, .3740] we'd predict label 0, but for the last output [0.2310, 0.7690] we'd predict label 1.  This is the same as choosing the largest number in each row in `out`.  We can use `torch.argmax` to return the index of the highest number in each row like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "584b51ad-8e6e-43f8-ad06-5b352786781e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.argmax( out, dim = 1 )\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb80581-1d2a-47d1-ac38-6a1c27ff67bc",
   "metadata": {},
   "source": [
    "So those are the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5845c-cc9b-44fd-88ee-1eaf0f864621",
   "metadata": {},
   "source": [
    "### Computing Accuracy on the Test Set\n",
    "\n",
    "Now that we have made the predictions for test data we can use any classification metric from scikit-learn.  In this case we'll compute accuracy.  Let's put everything into one cell to summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e42444fd-a587-4832-9681-58740c34c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 99.09\n"
     ]
    }
   ],
   "source": [
    "outputs,labels = predict_model(even_odd_model, test_loader)\n",
    "predictions = (outputs>0).long() # predictions = torch.argmax( out, dim = 1 ) if two classes\n",
    "accuracy = accuracy_score(labels, predictions) # truth, predictions\n",
    "print(f'The accuracy on the test set is {100*accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da20011-589c-4a5c-8669-5fcffbeb77da",
   "metadata": {},
   "source": [
    "We can also get all of the indices where the output and labels are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "342cb792-849b-47ff-b7ff-0be5b0f2805b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = torch.ne( predictions, labels ) # mostly they are equal so we'll see lots of Falses\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b052208a-7148-44c9-93b3-d3066c24709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  78,  124,  151,  209,  264,  359,  435,  479,  659,  813,  839, 1014,\n",
       "        1226, 1232, 1242, 1282, 1403, 1459, 1530, 1594, 1709, 1901, 1992, 2001,\n",
       "        2040, 2129, 2161, 2162, 2293, 2329, 2361, 2380, 2393, 2406, 2414, 2607,\n",
       "        2654, 2760, 3129, 3218, 3316, 3330, 3441, 3558, 3604, 3681, 3727, 3808,\n",
       "        3850, 3869, 3946, 3962, 4078, 4425, 4639, 4731, 4761, 4823, 4874, 5450,\n",
       "        5457, 5654, 5887, 5955, 5973, 5997, 6555, 6558, 6578, 6597, 6755, 7565,\n",
       "        7736, 7813, 7837, 7991, 8069, 8375, 9009, 9015, 9019, 9517, 9562, 9587,\n",
       "        9664, 9679, 9698, 9729, 9733, 9770, 9792])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.where(diff)[0] # get the index for each True\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a641c-6d95-4656-8828-7bb6beaa5c80",
   "metadata": {},
   "source": [
    "Now you can use the indices to inspect some of the instances that our model got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28debf60-1248-40e9-9314-a067c2594cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor, label = test_dataset[151]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30f2cf-5558-4efb-acd9-4c734a44bd5a",
   "metadata": {},
   "source": [
    "The label is 1 so that means it's on odd digit.  Our model must have predicted even. Let's have a look at the digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6634f4d8-dd3f-449c-8741-6eb5c640fd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2f5e52e10>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( image_tensor.squeeze(), cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7ba30-0bfc-4d99-85da-bd5a99c4595a",
   "metadata": {},
   "source": [
    "Maybe the model \"thought\" this 9 was an 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80285df5-872b-4d7d-b413-f245c299b338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
