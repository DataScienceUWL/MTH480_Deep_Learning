{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2623fa-4934-4c97-9074-d8632b998203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5342ab-b604-44ce-abc4-2e62b799c81b",
   "metadata": {},
   "source": [
    "# Preparing Names Data\n",
    "\n",
    "Here we'll review the steps for converting names to tensors and embeddings.  We'll also introduce the packed and padded sequences that make dealing with batches of sequences of different lengths possible.\n",
    "\n",
    "### 1. Preparing the Data\n",
    "\n",
    "Let's assume you have a dataset of names and their corresponding languages. Each name is a sequence of characters, and the names vary in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "304dfb06-6a9d-4255-ab98-1a792fd62bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Chen\", \"Smith\", \"Dubois\", \"O'Neill\", \"Kawasaki\", ]\n",
    "languages = [\"Chinese\", \"English\", \"French\", \"Irish\", \"Japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c784c1-b43f-4659-a6e3-474afe1924e3",
   "metadata": {},
   "source": [
    "## 2. Tokenizing and Encoding the Names\n",
    "Convert each name into a tensor of character indices. You'll need a character vocabulary for this.  \n",
    "\n",
    "* Tokenizing is breaking up text into smaller units like characters or words.\n",
    "* We can create our vocabulary using all the unique tokens in our corpus or a predefined vocabulary, such as ASCII characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5865660e-dba8-476d-a119-3de28f59bb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " ' ': 52,\n",
       " '.': 53,\n",
       " ',': 54,\n",
       " ';': 55,\n",
       " \"'\": 56}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the vocabulary\n",
    "\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "alphabet_to_index = { char:i for i,char in enumerate(all_letters) }  # this is the vocabulary\n",
    "alphabet_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eea949-4092-4bd0-a8de-17cd98248543",
   "metadata": {},
   "source": [
    "It will be helpful to have a function to convert each name to a tensor of its indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b45066-cc4d-4bda-bd36-88e3997938b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([12, 11, 14, 15]),\n",
       " tensor([16,  6, 10,  8, 11]),\n",
       " tensor([ 4, 17, 21,  7, 10,  9]),\n",
       " tensor([ 2,  1, 20, 14, 10,  5,  5]),\n",
       " tensor([ 3, 18, 13, 18,  9, 18, 19, 10])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode names as tensors of character indices\n",
    "def string_to_tensor(string, vocabulary):\n",
    "    indices = [vocabulary[char] for char in string]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "encoded_names = [string_to_tensor(name, char_to_index) for name in names]\n",
    "encoded_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64ee5f-60da-4b96-adee-6c251f4a8462",
   "metadata": {},
   "source": [
    "The output is a list of tensors of different lengths, but if we want to process a batch of sequences we'll need all the tensors to have the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403c05f-694a-446d-a724-54aaf62e7492",
   "metadata": {},
   "source": [
    "### 3. Padding the sequences\n",
    "\n",
    "Use `torch.nn.utils.rnn.pad_sequence` to pad the encoded names, making them all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "803210b4-1b81-4bf5-94e1-7ad1b46aa786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 11, 14, 15,  0,  0,  0,  0],\n",
       "        [16,  6, 10,  8, 11,  0,  0,  0],\n",
       "        [ 4, 17, 21,  7, 10,  9,  0,  0],\n",
       "        [ 2,  1, 20, 14, 10,  5,  5,  0],\n",
       "        [ 3, 18, 13, 18,  9, 18, 19, 10]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Pad the encoded names\n",
    "padded_names = pad_sequence(encoded_names, batch_first=True)\n",
    "padded_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13483328-d6cb-4327-bf30-1206375be236",
   "metadata": {},
   "source": [
    "In the book we're using the author also packs the sequences using when they're loaded by the dataloader, but then has to unpack them before creating the embeddings.  After the embeddings are created, they're repacked.  \n",
    "\n",
    "We'll show how packing works using our set of names, but in practice, we'll apply packing after the embedding step.\n",
    "\n",
    "**Note:**  If you search for information about packing and padding on the internet you'll find many resources saying the sequences should be sorted in order of descending length before packing and padding.  That's no longer necessary in PyTorch.  I've sorted the names in order of increasing length to make this easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e289d2-ec7f-443a-bae0-72f5a26e7e4b",
   "metadata": {},
   "source": [
    "#### Packing the sequences\n",
    "\n",
    "The best way to understand packing is to do it and view the results.  Note that we need the lengths of the sequences before packing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07060299-e14d-4556-9827-f577a18ac63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7, 8])\n",
      "PackedSequence(data=tensor([ 3,  2,  4, 16, 12, 18,  1, 17,  6, 11, 13, 20, 21, 10, 14, 18, 14,  7,\n",
      "         8, 15,  9, 10, 10, 11, 18,  5,  9, 19,  5, 10]), batch_sizes=tensor([5, 5, 5, 5, 4, 3, 2, 1]), sorted_indices=tensor([4, 3, 2, 1, 0]), unsorted_indices=tensor([4, 3, 2, 1, 0]))\n",
      "tensor([[12, 11, 14, 15,  0,  0,  0,  0],\n",
      "        [16,  6, 10,  8, 11,  0,  0,  0],\n",
      "        [ 4, 17, 21,  7, 10,  9,  0,  0],\n",
      "        [ 2,  1, 20, 14, 10,  5,  5,  0],\n",
      "        [ 3, 18, 13, 18,  9, 18, 19, 10]])\n"
     ]
    }
   ],
   "source": [
    "# Create a packed sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "lengths = torch.tensor([len(name) for name in encoded_names])\n",
    "print(lengths)\n",
    "\n",
    "packed_names = pack_padded_sequence(padded_names, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "print(packed_names)\n",
    "print(padded_names) # for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e733c-8ebb-488c-bd70-f8d971dd4d6f",
   "metadata": {},
   "source": [
    "For reference, PyTorch has a function for unpacking as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "233c02e5-1965-4732-a359-ad63c7544a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[12, 16,  4,  2,  3],\n",
      "        [11,  6, 17,  1, 18],\n",
      "        [14, 10, 21, 20, 13],\n",
      "        [15,  8,  7, 14, 18],\n",
      "        [ 0, 11, 10, 10,  9],\n",
      "        [ 0,  0,  9,  5, 18],\n",
      "        [ 0,  0,  0,  5, 19],\n",
      "        [ 0,  0,  0,  0, 10]]), tensor([4, 5, 6, 7, 8]))\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "unpacked_packed_names = pad_packed_sequence( packed_names )\n",
    "print(unpacked_packed_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126df9e-2c5a-4c72-ad40-4ff57d9b5702",
   "metadata": {},
   "source": [
    "Whoa, that isn't quite what I was expecting since the output is transposed (this is the format that RNN layers like), but we can fix that by specifying `batch_first = True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f0feaff-deb1-41d4-91f4-73b3559546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[12, 11, 14, 15,  0,  0,  0,  0],\n",
      "        [16,  6, 10,  8, 11,  0,  0,  0],\n",
      "        [ 4, 17, 21,  7, 10,  9,  0,  0],\n",
      "        [ 2,  1, 20, 14, 10,  5,  5,  0],\n",
      "        [ 3, 18, 13, 18,  9, 18, 19, 10]]), tensor([4, 5, 6, 7, 8]))\n"
     ]
    }
   ],
   "source": [
    "unpacked_packed_names = pad_packed_sequence( packed_names , batch_first = True)\n",
    "print(unpacked_packed_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db4da1-f12b-4342-9471-ae529b391915",
   "metadata": {},
   "source": [
    "### Embed the sequences\n",
    "\n",
    "Above we showed how to pack the padded sequences.  However, in practice we'll need sequences that are padded and not packed to create embeddings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba13fae1-1fc9-4cd9-9500-46ee40982043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 4])\n",
      "tensor([[[-0.6822, -0.9926,  1.7998, -1.4366],\n",
      "         [ 0.8385,  2.9592, -0.5322,  1.3943],\n",
      "         [-0.9660,  1.1546, -0.0527,  0.2459],\n",
      "         [ 2.2587,  0.3257, -0.9071,  0.0982],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691]],\n",
      "\n",
      "        [[ 0.8636, -1.5479,  0.9165, -0.9403],\n",
      "         [-1.2439,  0.0983,  0.3206, -0.8861],\n",
      "         [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
      "         [ 0.7767, -0.4849,  1.3134,  1.0431],\n",
      "         [ 0.8385,  2.9592, -0.5322,  1.3943],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691]],\n",
      "\n",
      "        [[-0.0987, -1.1727,  1.6411, -0.8086],\n",
      "         [ 0.3613,  0.4326, -2.0263, -1.6359],\n",
      "         [-0.0186,  0.0044,  1.9132, -1.0677],\n",
      "         [ 0.5504,  0.8048, -0.1656, -0.9748],\n",
      "         [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
      "         [-1.2631,  1.4951, -0.5700,  1.0441],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691]],\n",
      "\n",
      "        [[-0.3728, -0.2573,  0.1431, -1.7390],\n",
      "         [-1.1336,  0.7870,  0.1734,  1.0852],\n",
      "         [-0.7598,  1.7562, -0.5556, -0.1444],\n",
      "         [-0.9660,  1.1546, -0.0527,  0.2459],\n",
      "         [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
      "         [ 1.2733,  1.1936, -1.2952,  1.8277],\n",
      "         [ 1.2733,  1.1936, -1.2952,  1.8277],\n",
      "         [ 0.6579, -1.4150, -0.9433, -0.8691]],\n",
      "\n",
      "        [[-0.8737,  1.1373,  0.0240,  1.0491],\n",
      "         [-0.2080,  0.9390, -0.4297, -0.3330],\n",
      "         [-1.7522, -2.1198,  0.1935,  0.1146],\n",
      "         [-0.2080,  0.9390, -0.4297, -0.3330],\n",
      "         [-1.2631,  1.4951, -0.5700,  1.0441],\n",
      "         [-0.2080,  0.9390, -0.4297, -0.3330],\n",
      "         [ 0.9059, -0.7159,  1.9769, -0.4191],\n",
      "         [ 0.4721, -0.5125,  0.7859, -0.2357]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(alphabet_to_index)\n",
    "embedding_dim = 4 #for illustration, usually 64 to 256 or higher\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "embedded_names = embedding_layer(padded_names)\n",
    "\n",
    "print(embedded_names.shape)\n",
    "print(embedded_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8854e-643e-444a-893b-cf4d14817bcc",
   "metadata": {},
   "source": [
    "### 5.  Pack the embedded sequences before the RNN layer\n",
    "\n",
    "Everybody seems to do this, but I don't understand if it's actually helpful.  Let's try it to see if we can learn anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8b6f86d-33e9-4fbd-b3ec-55665e11f3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.8737,  1.1373,  0.0240,  1.0491],\n",
       "        [-0.3728, -0.2573,  0.1431, -1.7390],\n",
       "        [-0.0987, -1.1727,  1.6411, -0.8086],\n",
       "        [ 0.8636, -1.5479,  0.9165, -0.9403],\n",
       "        [-0.6822, -0.9926,  1.7998, -1.4366],\n",
       "        [-0.2080,  0.9390, -0.4297, -0.3330],\n",
       "        [-1.1336,  0.7870,  0.1734,  1.0852],\n",
       "        [ 0.3613,  0.4326, -2.0263, -1.6359],\n",
       "        [-1.2439,  0.0983,  0.3206, -0.8861],\n",
       "        [ 0.8385,  2.9592, -0.5322,  1.3943],\n",
       "        [-1.7522, -2.1198,  0.1935,  0.1146],\n",
       "        [-0.7598,  1.7562, -0.5556, -0.1444],\n",
       "        [-0.0186,  0.0044,  1.9132, -1.0677],\n",
       "        [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
       "        [-0.9660,  1.1546, -0.0527,  0.2459],\n",
       "        [-0.2080,  0.9390, -0.4297, -0.3330],\n",
       "        [-0.9660,  1.1546, -0.0527,  0.2459],\n",
       "        [ 0.5504,  0.8048, -0.1656, -0.9748],\n",
       "        [ 0.7767, -0.4849,  1.3134,  1.0431],\n",
       "        [ 2.2587,  0.3257, -0.9071,  0.0982],\n",
       "        [-1.2631,  1.4951, -0.5700,  1.0441],\n",
       "        [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
       "        [ 0.4721, -0.5125,  0.7859, -0.2357],\n",
       "        [ 0.8385,  2.9592, -0.5322,  1.3943],\n",
       "        [-0.2080,  0.9390, -0.4297, -0.3330],\n",
       "        [ 1.2733,  1.1936, -1.2952,  1.8277],\n",
       "        [-1.2631,  1.4951, -0.5700,  1.0441],\n",
       "        [ 0.9059, -0.7159,  1.9769, -0.4191],\n",
       "        [ 1.2733,  1.1936, -1.2952,  1.8277],\n",
       "        [ 0.4721, -0.5125,  0.7859, -0.2357]],\n",
       "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([5, 5, 5, 5, 4, 3, 2, 1]), sorted_indices=tensor([4, 3, 2, 1, 0]), unsorted_indices=tensor([4, 3, 2, 1, 0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "packed_embedded_names = pack_padded_sequence(embedded_names, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "packed_embedded_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbcfe7-10ff-42e6-b546-e8c5a54a2647",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
